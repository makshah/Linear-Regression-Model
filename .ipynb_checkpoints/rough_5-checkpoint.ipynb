{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of original data: (105, 3)\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "Theta obtained from normal equation: [-0.06234478  0.62017319  0.43647674]\n",
      "Residual sum of squares (RSS):  0.7590471383029536\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv # Used for computing the inverse of matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "data = np.loadtxt(open('sat_gpa.csv'), delimiter=',')\n",
    "print('shape of original data:', data.shape) # Check if data is 105 by 3\n",
    "\n",
    "# Normalize data\n",
    "data_norm = data / data.max(axis=0)\n",
    "X = np.ones_like(data_norm)\n",
    "print(X)\n",
    "X[:,1:3]=data_norm[:,0:2]\n",
    "##print(X)\n",
    "y= data_norm[:,-1]\n",
    "\n",
    "##print(y)\n",
    "#### END YOUR CODE ####\n",
    "\n",
    "\n",
    "# Compute theta using normal equation method\n",
    "# Hint: use the inv() function imported from numpy.linalg\n",
    "#### START YOUR CODE ####\n",
    "##X_t=np.transpose(X)\n",
    "#X_t=X.transpose()\n",
    "#m[]=np.dot(X_t,y)\n",
    "#d[]=np.dot(X_t,X)\n",
    "#i[]=inv(d)\n",
    "#theta_method1=np.dot(d,m)\n",
    "theta_method1 = np.dot((inv(np.dot((np.transpose(X)),X))),(np.dot((np.transpose(X)),y)))\n",
    "\n",
    "#### END YOUR CODE ####\n",
    "\n",
    "\n",
    "# Use the theta obtained to make predictions and compute the residuals\n",
    "# Hint: use numpy.dot() and numpy.sum(), and avoid using for loops\n",
    "#### START YOUR CODE ####\n",
    "y_hat =np.dot(X,theta_method1)\n",
    "#RSS1 = np.sum(np.square(y_hat-y))\n",
    "RSS1 = np.sum(np.square(np.subtract(y_hat,y)))\n",
    "\n",
    "#### END YOUR CODE ####\n",
    "\n",
    "# Compute residuals\n",
    "\n",
    "\n",
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "print('Theta obtained from normal equation:', theta_method1)\n",
    "print('Residual sum of squares (RSS): ', RSS1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of original data: (105, 3)\n",
      "[ 0.00167534 -0.00086071 -0.00112528]\n",
      "Theta obtained from gradient descent: [0.29911574 0.32224209 0.31267172]\n",
      "Residual sum of squares (RSS):  0.8641600584370602\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv # Used for computing the inverse of matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Use `pip install matplotlib` in command line if matplotlib is not installed\n",
    "\n",
    "# Load data \n",
    "data = np.loadtxt(open('sat_gpa.csv'), delimiter=',')\n",
    "print('shape of original data:', data.shape) # Check if data is 105 by 3\n",
    "\n",
    "# Normalize data\n",
    "data_norm = data / data.max(axis=0)\n",
    "\n",
    "# Define the gradientDescent function\n",
    "def gradientDescent(X, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "    Params\n",
    "        X - Shape: (m,3); m is the number of data examples\n",
    "        y - Shape: (m,)\n",
    "        theta - Shape: (3,)\n",
    "        num_iters - Maximum number of iterations\n",
    "    Return\n",
    "        A tuple: (theta, RSS, cost_array)\n",
    "        theta - the learned model parameters\n",
    "        RSS - residual sum of squares\n",
    "        cost_array - stores the cost value of each iteration. Its shape is (num_iters,)\n",
    "    '''\n",
    "    \n",
    "    m = len(y)\n",
    "    cost_array =[]\n",
    "    gradient=np.zeros(3)\n",
    "    #print(gradient)\n",
    "\n",
    "    for i in range(0, num_iters):\n",
    "        #### START YOUR CODE ####\n",
    "        # Make predictions\n",
    "        # Shape of y_hat: m by 1\n",
    "       ## y_hat =np.sum(theta,(np.dot(theta,X)))\n",
    "        y_hat=np.dot(X,theta)\n",
    "        # Compute the difference between prediction (y_hat) and ground truth label (y)\n",
    "        diff = np.subtract(y_hat,y)\n",
    "\n",
    "        # Compute the cost\n",
    "        # Hint: Use the diff computed above\n",
    "        #t=2*m\n",
    "        #cost = np.divide((np.sum(np.square(diff))),t)\n",
    "        cost = (1/2*m)*((np.sum(np.square(diff))))\n",
    "        #cost = (1/2*m)*(np.dot(diff.T,diff))\n",
    "        cost_array.append(cost)\n",
    "        \n",
    "        # Compute gradients\n",
    "        # Hint: Use the diff computed above\n",
    "        # Hint: Shape of gradients is the same as theta\n",
    "        ##gradient=np.dot((np.divide(np.sum(diff),m)),X)\n",
    "       \n",
    "       # g[0] = ((1/m)*(np.sum(diff))  \n",
    "       # g[1] = ((1/m)*(np.dot(np.sum(diff),X[1][1]))\n",
    "      #  gradient[2] = ((1/m)*(np.dot(np.sum(diff),X[1][2]))             \n",
    "       # k=0\n",
    "        #X_t=X.transpose()\n",
    "        #if(i<m):\n",
    "    \n",
    "         #   gradient[0] = ((1/m)*np.sum(diff))\n",
    "         #   gradient[1] = ((1/m)*(np.dot(np.sum(diff),X[i][1])))\n",
    "         #   gradient[2] = ((1/m)*(np.dot(np.sum(diff),X[i][2])))\n",
    "            \n",
    "        gradient=(np.dot(np.transpose(X),diff))/m\n",
    "        # Update theta\n",
    "        #if(i<3):\n",
    "       # theta = theta - (np.dot(alpha,gradient))\n",
    "        theta = theta - (alpha*(1/m)*np.dot(X.T,diff))\n",
    "       # theta = theta - (alpha*(1/m)*np.dot(X.T,diff))\n",
    "        #theta = theta - (alpha*(1/m)*np.dot(X.T,diff))\n",
    "        ##r=np.dot(np.divide(np.sum(diff),m),alpha)\n",
    "        ##theta= np.subtract(theta,r)\n",
    "        #s=np.dot(r,X)\n",
    "        #theta=\n",
    "        #### END YOUR CODE ####\n",
    "    print(gradient)\n",
    "    # Compute residuals\n",
    "    # Hint: Should use the same code as Task 1\n",
    "    #### START YOUR CODE ####\n",
    "    \n",
    "    y_hat =np.dot(X,theta)\n",
    "    RSS = np.sum(np.square(np.subtract(y_hat,y)))\n",
    "    #### END YOUR CODE ####\n",
    "\n",
    "    return theta, RSS, cost_array\n",
    "\n",
    "# This cell is to evaluate the gradientDescent function implemented above\n",
    "\n",
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "# Define learning rate and maximum iteration number\n",
    "ALPHA = 0.05\n",
    "MAX_ITER = 500\n",
    "# Initialize theta to [0,0,0]\n",
    "theta = np.zeros(3)\n",
    "\n",
    "theta_method2, RSS2, cost_array = gradientDescent(X, y, theta, ALPHA, MAX_ITER)\n",
    "\n",
    "print('Theta obtained from gradient descent:', theta_method2)\n",
    "print('Residual sum of squares (RSS): ', RSS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
